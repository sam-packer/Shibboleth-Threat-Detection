data:
  # Max rows to load from the database
  limit: 10000
  # Source table name in your database
  table: "rba_login_event"

  # Input features the model learns from
  feature_columns:
    - focus_changes
    - blur_events
    - click_count
    - key_count
    - avg_key_delay_ms
    - pointer_distance_px
    - pointer_event_count
    - scroll_distance_px
    - scroll_event_count
    - total_session_time_ms
    - time_to_first_key_ms
    - time_to_first_click_ms
    - idle_time_total_ms
    - input_focus_count
    - paste_events
    - resize_events
    - active_time_ms
    - tz_offset_min
    - device_memory_gb
    - hardware_concurrency
    - screen_width_px
    - screen_height_px
    - pixel_ratio
    - color_depth
    - touch_support
    - webauthn_supported

  # Fraction of training data used for validation (early stopping)
  validation_size: 0.2
  # Seed for reproducibility (same split every run)
  random_state: 41

preprocessing:
  # Directory to save preprocessing artifacts
  output_dir: "nn_data"

  # Concatenates with output_dir
  # Example: nn_data/rba_scaler.pkl
  artifacts:
    scaler: "rba_scaler.pkl"
    user_map: "rba_user_map.pkl"
    preprocessor: "rba_preprocessor.pkl"
    centroids: "rba_centroids.pkl"
    distance_stats: "rba_distance_stats.pkl"
    threshold: "threshold.txt"

model:
  # Directory to save model artifacts
  output_dir: "nn_data"
  # Concatenates with output_dir
  # Example: nn_data/best_rba_encoder.pt
  checkpoint: "best_rba_encoder.pt"

  # How many logins a user needs to have their own centroid
  min_user_events: 10
  # Minimum logins per (user, device_category) pair for a device-level centroid
  min_device_events: 10

  architecture:
    # Neurons per hidden layer
    hidden_layers: [64, 48]
    # Dimensionality of the output embedding
    embed_dim: 32
    # Fraction of neurons randomly disabled during training
    dropout: 0.15

training:
  # Maximum training iterations over the full dataset
  num_epochs: 500
  # Number of samples per training batch
  batch_size: 256
  # Margin for triplet loss (minimum separation between positive and negative pairs)
  triplet_margin: 0.3
  # Step size for weight updates (too high = unstable, too low = slow)
  learning_rate: 0.001
  # Stop if no improvement for this many epochs
  patience: 10
  # Minimum change to count as "improvement"
  min_delta: 1e-4
  runtime:
    # Hardware to train on
    device: "auto"  # auto | cuda | mps | cpu

mlflow:
  # Whether to log runs to MLFlow
  enable: true
  # Where to send tracking data
  tracking_uri: "databricks"
  # Where to register production models
  registry_uri: "databricks-uc"
  # Experiment location
  experiment_path: "/Shared/Shibboleth RBA/rba_model_training"
  # Unity Catalog name
  uc_catalog: "shibboleth_rba"
  # Schema within the catalog
  uc_schema: "models"
  # Registered model name
  uc_model_name: "Shibboleth_RBA_Model"

ensemble:
  # Where should MaxMind GeoIP data be stored?
  geoip_dir: "geoip_data"
  # Where should StopForumSpam data be stored?
  sfs_dir: "stopforumspam_data"

api:
  # IP address to bind to
  host: "127.0.0.1"
  # Port number for the API
  port: 5001

  # Note: Threads / workers are only configurable for Gunicorn on macOS / Linux
  # Number of server processes (parallelism)
  workers: 2
  # Threads per worker (concurrency)
  threads: 4
  # Verbosity: debug, info, warning, error
  log_level: "info"

# Do not change
config_version: "1"
